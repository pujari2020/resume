<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pradeep Pujari">
    <meta name="description" content="Pradeep Pujari | Data Engineer with 3+ Years Experience">
    <title>Pradeep Pujari | Projects</title>
    <link rel="stylesheet" href="style.css">
    <!-- Font Awesome for Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
</head>
<body>
    <div class="sidebar">
        <h2>Pradeep Pujari</h2>
        <p>Data Engineer | 3+ Years Experience</p>
        <ul>
            <li><a href="index.html"><i class="fas fa-home"></i> Home</a></li>
            <li><a href="career_summary.html"><i class="fas fa-briefcase"></i> Career Summary</a></li>
            <li><a href="technical_skills.html"><i class="fas fa-cogs"></i> Technical Skills</a></li>
            <li><a href="work_history.html"><i class="fas fa-history"></i> Work History</a></li>
            <li>
                <a href="projects.html" class="active"><i class="fas fa-project-diagram"></i> Projects</a>
                <ul class="submenu">
                    <li><a href="#project1"><i class="fas fa-credit-card"></i> Financial Intelligence Unit</a></li>
                    <li><a href="#project2"><i class="fas fa-exchange-alt"></i> Data Migration and Integration</a></li>
                </ul>
            </li>
            <li><a href="education.html"><i class="fas fa-graduation-cap"></i> Education</a></li>
            <li><a href="hobbies_soft_skills.html"><i class="fas fa-heart"></i> Hobbies & Soft Skills</a></li>
            <li><a href="contact.html"><i class="fas fa-address-book"></i> Contact</a></li>
        </ul>
    </div>

    <div class="main-content">
        <h1>My Projects</h1>

        <!-- Project 1 -->
        <section id="project1" class="project">
            <h2><i class="fas fa-credit-card"></i> Financial Intelligence Unit: On-Prem Data Processing and ETL Pipeline Development</h2>
            <p><strong>Description:</strong> Designed an ETL (Extract, Transform, Load) pipeline for handling credit cards and financial transactions data to determine rewards, EMI eligibility, discounts, and offers.</p>
            <p><strong>Tech Stacks:</strong> Spark, Hive, Scala, Python, SQL, Scala Spark, PySpark, SparkSQL, AWS -S3, EMR, Athena, Glue</p>
            <h4>Project Workflow:</h4>
            <ul>
                <li><strong>Data Ingestion:</strong> Ingested transactional data from financial systems (credit card transactions, bank transfers). Stored raw and processed data in Hive, HDFS, AWS S3.</li>
                <li><strong>Data Cataloging:</strong> Used Impala/Hue to scan data stored in Hive tables and AWS Glue Crawlers for Amazon S3 metadata.</li>
                <li><strong>Data Processing:</strong> Used Scala Spark, PySpark, SparkSQL, and Python on Jupyter Notebooks for data cleaning and aggregation.</li>
                <li><strong>Data Storage:</strong> Stored the transformed and enriched data back into Hive Tables / AWS S3 in a structured format (e.g., Parquet).</li>
                <li><strong>Data Querying and Analysis:</strong> Performed ad-hoc SQL queries on Hive Tables data with Impala/Hue and in AWS Athena.</li>
            </ul>
            <h4>Potential Outcomes:</h4>
            <ul>
                <li>Swiftly determined discounts, EMI eligibility, and rewards through credit card ETL pipelines.</li>
                <li>Optimized data transformation and storage with Spark techniques.</li>
            </ul>
        </section>

        <!-- Project 2 -->
        <section id="project2" class="project">
            <h2><i class="fas fa-exchange-alt"></i> Data Migration and Integration: On-Prem Data Processing and ETL Pipeline Development</h2>
            <p><strong>Description:</strong> Designed and implemented a robust ETL pipeline for data migration and integration into a centralized data repository, leveraging on-premises data computing capabilities.</p>
            <p><strong>Tech Stacks:</strong> SQL, Python, Scala, PySpark, SparkSQL, SQOOP, Hive, HBase, Cron Jobs, Shell Scripts</p>
            <h4>Project Workflow:</h4>
            <ul>
                <li><strong>Data Ingestion:</strong> Used Sqoop/PySpark to import data from RDBMS to HDFS and ingested semi-structured data into Hive tables.</li>
                <li><strong>Data Processing:</strong> Utilized PySpark, SparkSQL, and Hive for data transformation and aggregation.</li>
                <li><strong>Data Storage:</strong> Stored raw and transformed data in HDFS and Hive for structured querying and in HBase for low-latency access.</li>
                <li><strong>Scheduling:</strong> Employed Cron Jobs/Airflow for task scheduling and pipeline orchestration.</li>
                <li><strong>Version Control & Deployment:</strong> Managed code using Git/GitHub and implemented CI/CD pipelines for automated testing and deployment.</li>
            </ul>
            <h4>Potential Outcomes:</h4>
            <ul>
                <li>Developed a robust ETL pipeline for efficient data migration and integration.</li>
                <li>Enhanced data computing performance with PySpark in-memory computations.</li>
                <li>Structured data storage in Hive and real-time access facilitated by HBase.</li>
            </ul>
        </section>
    </div>
</body>
</html>
