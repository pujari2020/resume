<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pradeep Pujari">
    <meta name="description" content="Showcasing Pradeep Pujari's data engineering projects, including ETL pipeline development and big data solutions using Spark, Hive, and AWS.">
    <meta name="keywords" content="Data Engineering, Projects, Spark, Hive, AWS, ETL Development">
    <title>Projects | Pradeep Pujari</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet" loading="lazy">
</head>
<body>
    <div class="sidebar">
        <h2>Pradeep Pujari</h2>
        <p>Data Engineer | 3+ Years Experience</p>
        <ul>
            <li><a href="index.html"><i class="fas fa-home"></i> Home</a></li>
            <li><a href="career_summary.html"><i class="fas fa-briefcase"></i> Career Summary</a></li>
            <li><a href="technical_skills.html"><i class="fas fa-cogs"></i> Technical Skills</a></li>
            <li><a href="work_history.html"><i class="fas fa-history"></i> Work History</a></li>
            <li><a href="projects.html" class="active"><i class="fas fa-project-diagram"></i> Projects</a></li>
            <li><a href="education.html"><i class="fas fa-graduation-cap"></i> Education</a></li>
            <li><a href="hobbies_soft_skills.html"><i class="fas fa-heart"></i> Hobbies & Soft Skills</a></li>
            <li><a href="contact.html"><i class="fas fa-address-book"></i> Contact</a></li>
        </ul>
    </div>

    <div class="main-content">
        <h1>Projects</h1>

        <!-- Project 1 -->
        <article class="project">
            <h2><i class="fas fa-credit-card"></i> Financial Intelligence Unit: On-Prem Data Processing and ETL Pipeline Development</h2>
            <p><strong>Description:</strong> Designed an ETL pipeline to process financial transactions for determining rewards, EMI eligibility, discounts, and offers.</p>
            <p><strong>Tech Stacks:</strong> Spark, Hive, Scala, Python, SQL, PySpark, SparkSQL, AWS-S3, EMR, Athena, Glue</p>

            <h3>Workflow:</h3>
            <ul>
                <li><strong>Data Ingestion:</strong> Transactional data ingested from financial systems into Hive, HDFS, and AWS S3.</li>
                <li><strong>Data Cataloging:</strong> Employed Impala/Hue and AWS Glue Crawlers for metadata discovery and cataloging.</li>
                <li><strong>Data Processing:</strong> Data cleaned, transformed, and aggregated using Scala Spark, PySpark, SparkSQL, and Python.</li>
                <li><strong>Data Storage:</strong> Enriched data stored in Hive Tables and AWS S3 (Parquet format) for efficient querying.</li>
                <li><strong>Data Querying and Analysis:</strong> Insights derived using Impala, Hue, and Athena for ad-hoc queries.</li>
            </ul>

            <h3>Outcomes:</h3>
            <ul>
                <li>Enhanced speed and accuracy in determining discounts, EMI eligibility, and rewards.</li>
                <li>Optimized data processing and storage through Spark techniques.</li>
            </ul>
        </article>

        <!-- Project 2 -->
        <article class="project">
            <h2><i class="fas fa-exchange-alt"></i> Data Migration and Integration: On-Prem Data Processing and ETL Pipeline Development</h2>
            <p><strong>Description:</strong> Built and implemented an ETL pipeline for migrating and integrating data into a centralized repository.</p>
            <p><strong>Tech Stacks:</strong> SQL, Python, Scala, PySpark, SparkSQL, SQOOP, Hive, HBase, Cron Jobs, Shell Scripts</p>

            <h3>Workflow:</h3>
            <ul>
                <li><strong>Data Ingestion:</strong> Imported data from RDBMS to HDFS using Sqoop/PySpark and ingested semi-structured data into Hive tables.</li>
                <li><strong>Data Processing:</strong> Optimized transformations and aggregations using PySpark, SparkSQL, and Hive.</li>
                <li><strong>Data Storage:</strong> Raw and processed data stored in HDFS, Hive, and HBase for structured querying and low-latency access.</li>
                <li><strong>Scheduling:</strong> Automated workflows with Cron Jobs and Airflow.</li>
                <li><strong>Version Control:</strong> Code managed with Git/GitHub and deployed via CI/CD pipelines.</li>
            </ul>

            <h3>Outcomes:</h3>
            <ul>
                <li>Successfully migrated and integrated data from multiple RDBMS sources.</li>
                <li>Enhanced performance through PySpark in-memory computations.</li>
                <li>Enabled real-time data access with HBase.</li>
            </ul>
        </article>

        <!-- Add more projects as needed -->
    </div>
</body>
</html>
